{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1am4Hpv-8fqxPfXhOuGLpb6KHVl85rASy",
      "authorship_tag": "ABX9TyMmHHQxzHfo8vTMoKfDm/ix",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluevit/NER-Testing/blob/main/NER_for_ResumeAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Training spaCy NER for Resume Analysis</h3>\n"
      ],
      "metadata": {
        "id": "aRF5YRktW07P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOYpCT9v9RBd",
        "outputId": "cd376c0b-0ef5-4387-feef-6c5b3d0ae061"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.25 in /usr/local/lib/python3.10/dist-packages (1.25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy==3.7.2\n",
        "!pip install spacy-transformers"
      ],
      "metadata": {
        "id": "pv_CRv5cm7Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "SgpoPgHMnl1F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/laxmimerit/CV-Parsing-using-Spacy-3.git"
      ],
      "metadata": {
        "id": "glaUB9zsolEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3efab2d7-2ca2-438e-ad77-7b2a899e6c1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CV-Parsing-using-Spacy-3'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 82 (delta 14), reused 74 (delta 14), pack-reused 6 (from 1)\u001b[K\n",
            "Receiving objects: 100% (82/82), 5.62 MiB | 20.99 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "cv_data = json.load(open('/content/CV-Parsing-using-Spacy-3/data/training/train_data.json','r'))"
      ],
      "metadata": {
        "id": "U00nOGF6n2vk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cv_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6QlMtdopRVH",
        "outputId": "216037a4-0d2c-48a7-8c27-b40d66ff2315"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you can see the data(json) by uncommenting below statement\n",
        "#cv_data"
      ],
      "metadata": {
        "id": "ftB0dw1rattX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config /content/CV-Parsing-using-Spacy-3/data/training/base_config.cfg /content/CV-Parsing-using-Spacy-3/data/training/config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biYUMq1spsd1",
        "outputId": "e718e851-16a7-4dd3-fca6-a124541438b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "/content/CV-Parsing-using-Spacy-3/data/training/config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "config.cfg must be modified in order to reduce the training time<br>\n",
        "(generated in *content/CV-Parsing-using-Spacy-3/data/training/config.cfg*)<br><br>\n",
        "max_steps parameter must be set to some low number like 2000 or 4000, which will drastically reduce the training time.</h3>\n",
        "<hr>"
      ],
      "metadata": {
        "id": "sCv9ydcoYhV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes a dataset of text and annotations, processes each text to identify and label entities, and stores the results in a DocBin object that can be used for further training or analysis with spaCy."
      ],
      "metadata": {
        "id": "3OJ7_SGfamRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spacy_doc(file, data):\n",
        "  nlp = spacy.blank('en')\n",
        "  db = DocBin()\n",
        "  for text, annot in tqdm(data):\n",
        "    doc = nlp.make_doc(text)\n",
        "    annot = annot['entities']\n",
        "\n",
        "    ents = []\n",
        "    entity_indices = []\n",
        "\n",
        "    for start, end, label in annot:\n",
        "      skip_entity = False\n",
        "      for idx in range(start, end):\n",
        "        if idx in entity_indices:\n",
        "          skip_entity = True\n",
        "          break\n",
        "      if skip_entity == True:\n",
        "        continue\n",
        "      entity_indices = entity_indices + list(range(start, end))\n",
        "      try:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "      if span is None:\n",
        "        err_data = str([start, end]) + ' ' + str(text) + '\\n'\n",
        "        file.write(err_data)\n",
        "      else :\n",
        "        ents.append(span)\n",
        "    try:\n",
        "      doc.ents = ents\n",
        "      db.add(doc)\n",
        "    except:\n",
        "      pass\n",
        "      #print(text)\n",
        "  return db"
      ],
      "metadata": {
        "id": "z54oCPNspsaZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(cv_data, test_size=0.3)"
      ],
      "metadata": {
        "id": "jsxMgzn0psYy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('error.txt', 'w')\n",
        "db = get_spacy_doc(file, train)\n",
        "db.to_disk('train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(file, test)\n",
        "db.to_disk('test_data.spacy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frBj3E2HpsWE",
        "outputId": "ccc80d67-dbb3-47c6-c88d-f40722e098e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "100%|██████████| 140/140 [00:02<00:00, 64.42it/s]\n",
            "100%|██████████| 60/60 [00:00<00:00, 67.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /content/CV-Parsing-using-Spacy-3/data/training/config.cfg --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXSbjy3IpsTW",
        "outputId": "10d62f60-c6bc-473d-c5a4-d4ef7d238bab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 158kB/s]\n",
            "config.json: 100% 481/481 [00:00<00:00, 3.19MB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 1.03MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 2.08MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 35.4MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "model.safetensors: 100% 499M/499M [00:04<00:00, 114MB/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "/usr/local/lib/python3.10/dist-packages/thinc/shims/pytorch.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
            "  0       0        5367.85   1213.60    0.18    0.11    0.46    0.00\n",
            "  3     200      392852.23  86695.00   11.66   55.88    6.51    0.12\n",
            "  7     400       48923.64  32270.68   53.92   55.80   52.17    0.54\n",
            " 11     600       22417.23  29563.80   59.39   62.58   56.51    0.59\n",
            " 14     800       26328.06  26031.15   55.84   62.86   50.23    0.56\n",
            " 18    1000       28667.10  25734.45   56.57   62.28   51.83    0.57\n",
            " 22    1200        3073.12  25012.46   56.76   57.49   56.05    0.57\n",
            " 25    1400        1140.24  23711.23   56.33   57.09   55.59    0.56\n",
            " 29    1600         634.85  22329.86   58.21   66.37   51.83    0.58\n",
            " 33    1800        2135.68  23128.28   49.11   70.13   37.79    0.49\n",
            " 37    2000        1970.10  22963.20   54.39   50.44   59.02    0.54\n",
            " 40    2200        6406.60  21200.54   58.10   63.67   53.42    0.58\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Testing the Trained Model</h3>"
      ],
      "metadata": {
        "id": "HA2OcJgxHBr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('/content/output/model-best')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzwZDEUjpsPc",
        "outputId": "5e522269-68db-4f38-cfee-8b7ea1b61bd3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy_transformers/layers/hf_shim.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self._model.load_state_dict(torch.load(filelike, map_location=device))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXhYafFtpsJ9",
        "outputId": "e5d59129-0c69-41d2-bf54-c02d2176b0c6",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.11-cp38-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading PyMuPDF-1.24.11-cp38-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.6 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/19.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/19.6 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/19.6 MB\u001b[0m \u001b[31m135.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m14.9/19.6 MB\u001b[0m \u001b[31m155.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m177.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m177.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys , fitz"
      ],
      "metadata": {
        "id": "9qO5hHqnpsIg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname = '/content/CV-Parsing-using-Spacy-3/data/test/Alice Clark CV.pdf'\n",
        "doc = fitz.open(fname)\n",
        "text = \"\"\n",
        "for page in doc:\n",
        "  text = text + str(page.get_text())\n",
        "doc.close()"
      ],
      "metadata": {
        "id": "VHKQp7vtpsEV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.strip()"
      ],
      "metadata": {
        "id": "LjIxwf1YpsCy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "zy1-dVU8pr-3",
        "outputId": "99aa2359-c0df-4022-90be-03afba766e2d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Alice Clark \\nAI / Machine Learning \\n \\nDelhi, India Email me on Indeed \\n• \\n20+ years of experience in data handling, design, and development \\n• \\nData Warehouse: Data analysis, star/snow flake scema data modelling and design specific to \\ndata warehousing and business intelligence \\n• \\nDatabase: Experience in database designing, scalability, back-up and recovery, writing and \\noptimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes. \\nCloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure, \\nStream Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake \\nanalytics(U-SQL) \\nWilling to relocate anywhere \\n \\nWORK EXPERIENCE \\nSoftware Engineer \\nMicrosoft – Bangalore, Karnataka \\nJanuary 2000 to Present \\n1. Microsoft Rewards Live dashboards: \\nDescription: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping \\nonline. Microsoft Rewards members can earn points when searching with Bing, browsing with \\nMicrosoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft \\nStore. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft \\nrewards website. Rewards live dashboards gives a live picture of usage world-wide and by \\nmarkets like US, Canada, Australia, new user registration count, top/bottom performing rewards \\noffers, orders stats and weekly trends of user activities, orders and new user registrations. the \\nPBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes. \\nTechnology/Tools used \\n \\nEDUCATION \\nIndian Institute of Technology – Mumbai \\n2001 \\n \\nSKILLS \\nMachine Learning, Natural Language Processing, and Big Data Handling \\n \\nADDITIONAL INFORMATION \\nProfessional Skills \\n• Excellent analytical, problem solving, communication, knowledge transfer and interpersonal \\nskills with ability to interact with individuals at all the levels \\n• Quick learner and maintains cordial relationship with project manager and team members and \\ngood performer both in team and independent job environments \\n• Positive attitude towards superiors &amp; peers \\n• Supervised junior developers throughout project lifecycle and provided technical assistance'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, '---->' , ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7d36gGPpr9h",
        "outputId": "7e29b0fb-cd09-42c9-b4c6-dc7a2955104f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice Clark ----> Name\n",
            "Delhi ----> Location\n",
            "Microsoft ----> Companies worked at\n",
            "Software Engineer ----> Designation\n",
            "Microsoft ----> Companies worked at\n",
            "Microsoft ----> Companies worked at\n",
            "Microsoft ----> Companies worked at\n",
            "Microsoft ----> Companies worked at\n",
            "Microsoft ----> Companies worked at\n",
            "Indian Institute of Technology ----> College Name\n",
            "Machine Learning, Natural Language Processing, and Big Data Handling ----> Skills\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/output.zip /content/output"
      ],
      "metadata": {
        "id": "lD2teLSpprzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "4d5d7abb-5c05-4ea7-de8d-ff834965ee2e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/output/ (stored 0%)\n",
            "  adding: content/output/model-best/ (stored 0%)\n",
            "  adding: content/output/model-best/config.cfg (deflated 61%)\n",
            "  adding: content/output/model-best/ner/ (stored 0%)\n",
            "  adding: content/output/model-best/ner/model (deflated 8%)\n",
            "  adding: content/output/model-best/ner/cfg (deflated 33%)\n",
            "  adding: content/output/model-best/ner/moves (deflated 73%)\n",
            "  adding: content/output/model-best/meta.json (deflated 66%)\n",
            "  adding: content/output/model-best/transformer/ (stored 0%)\n",
            "  adding: content/output/model-best/transformer/model (deflated 13%)\n",
            "  adding: content/output/model-best/transformer/cfg (stored 0%)\n",
            "  adding: content/output/model-best/tokenizer (deflated 81%)\n",
            "  adding: content/output/model-best/vocab/ (stored 0%)\n",
            "  adding: content/output/model-best/vocab/strings.json (deflated 76%)\n",
            "  adding: content/output/model-best/vocab/key2row (stored 0%)\n",
            "  adding: content/output/model-best/vocab/vectors.cfg (stored 0%)\n",
            "  adding: content/output/model-best/vocab/vectors (deflated 45%)\n",
            "  adding: content/output/model-best/vocab/lookups.bin (stored 0%)\n",
            "  adding: content/output/model-last/ (stored 0%)\n",
            "  adding: content/output/model-last/config.cfg (deflated 61%)\n",
            "  adding: content/output/model-last/ner/ (stored 0%)\n",
            "  adding: content/output/model-last/ner/model (deflated 8%)\n",
            "  adding: content/output/model-last/ner/cfg (deflated 33%)\n",
            "  adding: content/output/model-last/ner/moves (deflated 73%)\n",
            "  adding: content/output/model-last/meta.json (deflated 66%)\n",
            "  adding: content/output/model-last/transformer/ (stored 0%)\n",
            "  adding: content/output/model-last/transformer/model (deflated 13%)\n",
            "  adding: content/output/model-last/transformer/cfg (stored 0%)\n",
            "  adding: content/output/model-last/tokenizer (deflated 81%)\n",
            "  adding: content/output/model-last/vocab/ (stored 0%)\n",
            "  adding: content/output/model-last/vocab/strings.json (deflated 76%)\n",
            "  adding: content/output/model-last/vocab/key2row (stored 0%)\n",
            "  adding: content/output/model-last/vocab/vectors.cfg (stored 0%)\n",
            "  adding: content/output/model-last/vocab/vectors (deflated 45%)\n",
            "  adding: content/output/model-last/vocab/lookups.bin (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<h4>For Reference:</h4>\n",
        "<a href='https://github.com/laxmimerit/CV-Parsing-using-Spacy-3.git'>CV-Parsing-using-Spacy-3\n",
        "</a><br>\n",
        "<a href='https://github.com/yashlikescode/spacyResumeParcer.git'>spacyResumeParcer</a>\n",
        "<hr>"
      ],
      "metadata": {
        "id": "wzQ1mwz6YIwP"
      }
    }
  ]
}